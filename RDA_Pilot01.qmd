---
title: "Effsize Text vs. Viz - Pilot Study I"
subtitle: "Reproducible Documentation of Analysis"
format: 
  html:
    toc: true
    theme: flatly
    fontsize: .8em
editor_options: 
  chunk_output_type: console
lightbox: true  
---

## Import of the already cleaned data

```{r}
library(tidyverse)
library(GGally)
library(skimr)
library(lubridate)
library(hrbrthemes)
library(effectsize)
library(brms)
library(ggforce)

data_pilot01 <- 
  read_delim("Data/TextViz.csv", delim = ";")|> 
  # compute survey duration
  mutate(survey_duration = dmy_hm(ended) - dmy_hm(created)) |> 
  # rename true effect sizes
  mutate(text_effect_size_1 = effect_size_7,
         text_effect_size_2 = effect_size_8,
         text_effect_size_3 = effect_size_9,
         text_effect_size_4 = effect_size_10,
         text_effect_size_5 = effect_size_11,
         text_effect_size_6 = effect_size_12,
         plot_effect_size_1 = effect_size_1,
         plot_effect_size_2 = effect_size_2,
         plot_effect_size_3 = effect_size_3,
         plot_effect_size_4 = effect_size_4,
         plot_effect_size_5 = effect_size_5,
         plot_effect_size_6 = effect_size_6) |> 
  # deselect old true effect size variables
  select(-starts_with("effect_size_"))

glimpse(data_pilot01)
skimr::skim(data_pilot01)
```

## Participant Behaviour

### Survey Duration

```{r}
data_pilot01  |> 
  ggplot(aes(survey_duration)) +
  geom_histogram() +
  theme_minimal()

```

## Results of Attention and Comprehension checks

```{r}
data_pilot01 |> 
  select(attention_check_1a, attention_check_1b,
         check_graph2, check_test, check_text2) |> 
  skim()
```

## Data Wrangling Wide to Long Format

It is important to know that `effect_size1` to `effect_size6` encode the true effect sizes of the shown plots. `effect_size7` to `effect_size12` encode the true effect sizes of the shown **text**.

First we have to wrangle a table like this:

| session | stimulus_type | stimulus_number | true_effectsize | first_stim | perceived_diff | perceived_PS | perceived_inf | perceived_rel |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| a       | plot          | 1               | .8              | plot_first |                |              |               |               |
| b       | text          | 1               | .2              | text_first |                |              |               |               |
| c       | plot          | 1               | -.5             | plot_first |                |              |               |               |

The variable `first_stim` is only reconstuctable by the time stamps. So let's wrangle the first four columns:

```{r}
data_maxlong <- 
  data_pilot01 |> 
  select(session,
         starts_with("plot_"),
         starts_with("text_")) |> 
  pivot_longer(cols = -session,
               names_to = "variable",
               values_to = "value") |> 
  mutate(stimulus_type = str_sub(variable, 1, 4),
         nth_stimulus_per_type = str_sub(variable, -2, -1),
         nth_stimulus_per_type = case_when(nth_stimulus_per_type == "_1" ~ 1,
                                           nth_stimulus_per_type == "_2" ~ 2,
                                           nth_stimulus_per_type == "_3" ~ 3,
                                           nth_stimulus_per_type == "_4" ~ 4,
                                           nth_stimulus_per_type == "_5" ~ 5,
                                           nth_stimulus_per_type == "_6" ~ 6,
                                           nth_stimulus_per_type == "_7" ~ 7,
                                           nth_stimulus_per_type == "_8" ~ 8,
                                           nth_stimulus_per_type == "_9" ~ 9,
                                           nth_stimulus_per_type == "10" ~ 10,
                                           nth_stimulus_per_type == "11" ~ 11,
                                           nth_stimulus_per_type == "12" ~ 12), 
         measurement = case_when(str_detect(variable, "accuracy") ~ "probability_of_sup",
                                 str_detect(variable, "effect_size") ~ "effect_size",
                                 str_detect(variable, "informativity") ~ "informativity",
                                 str_detect(variable, "relevance") ~ "relevance",
                                 str_detect(variable, "difficulty") ~ "difficulty",
                                 T ~ "Attention: Error in Code"))

data_long <- 
  data_maxlong |> 
  select(-variable) |> 
  pivot_wider(id_cols = c(session, stimulus_type, nth_stimulus_per_type),
              names_from = measurement,
              values_from = value) 

skim(data_long)  
```

## Research Questions

RQ 1: How accurate (dependent variable 1), informative (dependent variable 2), difficult to understand / efficient (dependent variable 3) and relevant (dependent variable 4) are graphically and textually presented effect sizes perceived by teachers?

RQ 2: Are there any (significant) differences between the two modes (graph vs. text) in relation to the four dependent variables?

## Statistical Analyses

Concerning Research Question 1, part

1: First step: we need to transform the variable `effect size`, which is computed as Cohen's d values, into the Common Language Effect Size (CLES) Probability of Superiority. Afterward, we can calculate a Pearson's r correlation. We need a table like this:

| effect_size_true | pos_effect_size_true | probability of superiority (measurement) |
|------------------|-------------------|-----------------------------------|
| +- 0.8           | 71.4                 | ...                                      |
| +- 0.5           | 63.8                 | ...                                      |
| +- 0.2           | 55.6                 | ...                                      |

: Correlation between effect_size and probability of superiority

## Transformation of accuracy variable

```{r}
# reminder to myself: AltGr + greater symbol = pipe operator


data_long <- 
  data_long |> 
  # we recognized that our stimulus is the other way around, that's why we
  # have to recode the effect sizes variable
  mutate(#effect_size_pos = d_to_p_superiority(effect_size), 
         #effect_size_pos = 1-(effect_size_pos),
         #effect_size_pos = 100*(effect_size_pos),
         #probability_of_sup_diff = probability_of_sup - effect_size_pos,
         rating_in_d = sqrt(2)*qnorm(probability_of_sup/100),
         overunderrating = ifelse(effect_size < 0, 
                                  (rating_in_d - effect_size)*(-1), # () f체r Flo
                                  rating_in_d - effect_size),
         relevance_obj = case_when(effect_size > 0 ~ 0,
                                   T ~ abs(effect_size))) |> 
  # person specific standardization of relevance
  group_by(session) |> 
  mutate(relevance_pstand = (relevance - mean(relevance, na.rm = T))/
           sd(relevance, na.rm = T)) |> 
  ungroup()
```

## Analysis per dependent variables
### Informativity
#### Plot

```{r}
data_long |>
  ggplot(aes(stimulus_type, informativity)) +
  geom_violin() +
  geom_sina() +
  stat_summary(fun.data = "mean_sdl",
               fun.args = list(mult = 1)) +
  theme_minimal() +
  theme(legend.position = "none")
```

#### Model
```{r}
mod0_informativity <- brm(informativity ~ + (1|session), 
                          data = data_long,
                          cores = 4)

mod1_informativity <- brm(informativity ~ stimulus_type + (1|session), 
                          data = data_long,
                          iter = 40000,
                          cores = 4)


summary(mod1_informativity)
pp_check(mod1_informativity)
hypothesis(mod1_informativity, "stimulus_typetext > 0")

# sjPlot::tab_model(mod0_informativity, mod1_informativity, mod2_informativity)

```

#### Effect size 
```{r}
cliffs_delta(data_long$stimulus_type, data_long$informativity)
```


### Accuracy: Over/underatings

#### Plot

```{r}
# Overunderrating per stimulus type
data_long |> 
    ggplot(aes(stimulus_type, overunderrating)) +
    geom_boxplot() +
    geom_jitter() +
    stat_smooth() +
    theme_minimal()

data_long |> 
  ggplot(aes(overunderrating)) +
  geom_histogram() +
  facet_wrap(~stimulus_type)
```


#### Model

```{r}
mod0_overunderrating <- brm(overunderrating ~ + (1|session), 
                          data = data_long,
                          cores = 4)

# Error: Exception: normal_id_glm_lpdf: Vector of dependent variables[159] is -inf, but must be finite!  (in 'model14bf34ad92f8_6f14f045693eab537b22cadd2fdca599' at line 53)
data_overunderrating <- data_long |>
  filter(!overunderrating == "-Inf")

mod1_overunderrating <- brm(overunderrating ~ stimulus_type + (1|session), 
                          data = data_overunderrating,
                          iter = 40000,
                          cores = 4)


summary(mod1_overunderrating)
pp_check(mod1_overunderrating)
hypothesis(mod1_overunderrating, "Intercept < 0")
hypothesis(mod1_overunderrating, "stimulus_typetext < 0")

# sjPlot::tab_model(mod0_informativity, mod1_informativity, mod2_informativity)

```

#### Effect size 
```{r}
data_cliffs <- data_long |>
  mutate(stimulus_type = as.factor(stimulus_type))

cliffs_delta(stimulus_type ~ overunderrating, data = data_cliffs)
```


### Difficulty

#### Plot

```{r}
data_long |>
  ggplot(aes(stimulus_type, difficulty)) +
  geom_violin() +
  geom_sina(aes(color = session)) +
  stat_summary(
    fun.data = "mean_sdl",
    fun.args = list(mult = 1)
  ) + 
  theme_minimal() +
  theme(legend.position = "none")

testmod

```

#### Model

```{r}

```


#### Effect Size

```{r}

```

### Relevance

#### Plot

```{r}
data_long |> 
    ggplot(aes(stimulus_type, relevance)) +
    geom_jitter() +
    theme_minimal()

# it seems like some participants have always used the same number 
# thats why we plot relevance per condition and person
data_long |>
  ggplot(aes(stimulus_type, 
             relevance, 
             color = session)) +
  geom_jitter(alpha = .6) +
  facet_wrap(~effect_size) + 
  theme_minimal() +
  theme(legend.position = "none")
```

##### Log Relevance per condition

```{r}
data_long |>
  ggplot(aes(stimulus_type, 
             log(relevance))) +
  geom_jitter(aes(color = session),
              alpha = .6, width = .1) +
  stat_summary(fun.data = "mean_sdl",
               fun.args = list(mult = 1)) +
  facet_wrap(~effect_size) + 
  theme_minimal() +
  theme(legend.position = "none")
```

##### Relevance per nth-stimulus

```{r}
data_long |> 
  ggplot(aes(nth_stimulus_per_type, 
             log(relevance +1), 
             color = session,
             group = session)) +
  geom_point(aes(size = relevance_obj)) +
  geom_line() +
  facet_wrap(~ stimulus_type) +
  theme_minimal() +
  theme(legend.position = "none")
```

##### Personwise standadized relevancee per true effect size

```{r}
data_long |>
  ggplot(aes(stimulus_type, 
             relevance_pstand)) +
  geom_jitter(alpha = .6, width = .1) +
  stat_summary(fun.data = "mean_sdl",
               fun.args = list(mult = 1)) +
  facet_wrap(~effect_size) + 
  geom_hline(yintercept = 0, color = "#8cd000") +
  theme_minimal()

data_long |> 
  ggplot(aes(relevance_obj, relevance_pstand)) + 
  geom_jitter() +
  stat_smooth() +
  theme_minimal()
```

Comment: relevance seems to be difficult - some people have no variance in their estimates and there is no clear pattern in the data

```{r}
data_long |> 
    group_by(stimulus_type) |> 
    do(tau_accuracy = unlist(cor(.$effect_sizes, .$probability_of_sup, method = "kendall", 
                           use = "pairwise.complete"))) %>% 
    unnest(tau_accuracy)
```

#### Model 

```{r}

mod1_relevance_pstand <- brm(relevance_pstand ~ relevance_obj + (1|session), 
                          data = data_long,
                          iter = 40000,
                          cores = 4)

summary(mod1_relevance_pstand)
pp_check(mod1_relevance_pstand)


## stimulus type: text

data_long |> 
  filter(stimulus_type == "text") |>
  ggplot(aes(relevance_obj, relevance_pstand)) + 
  geom_jitter() +
  stat_smooth() +
  theme_minimal()

mod1_relevance_pstandtext <- brm(relevance_pstand ~ relevance_obj + (1|session), 
                          data = data_long |> 
                            filter(stimulus_type == "text"),
                          iter = 40000,
                          cores = 4)

summary(mod1_relevance_pstandtext)
pp_check(mod1_relevance_pstandtext)

## stimulus type: plot

data_long |> 
  filter(stimulus_type == "plot") |>
  ggplot(aes(relevance_obj, relevance_pstand)) + 
  geom_jitter() +
  stat_smooth() +
  theme_minimal()

mod1_relevance_pstandplot <- brm(relevance_pstand ~ relevance_obj + (1|session), 
                          data = data_long |> 
                            filter(stimulus_type == "plot"),
                          iter = 40000,
                          cores = 4)

summary(mod1_relevance_pstandplot)
pp_check(mod1_relevance_pstandplot)




hypothesis(mod1_relevance_pstand, "relevance_obj > 0")
```

Comment: stimulus type nicht ber체cksichtig, gew채hlte Variante vermutlich nicht sinnvoll. Was w채re hier eine sinnige Hypothese

#### Effect Size 

```{r}

```

