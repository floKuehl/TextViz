---
title: "Effsize Text vs. Viz - Pilot Study I"
subtitle: "Reproducible Documentation of Analysis (RDA)"
format: 
  html:
    toc: true
    theme: flatly
    fontsize: .8em
editor_options: 
  chunk_output_type: console
lightbox: true  
---

## Import of the already cleaned data

```{r}
library(tidyverse)
library(GGally)
library(skimr)
library(lubridate)
library(hrbrthemes)
library(effectsize)
library(brms)
library(ggforce)
options(mc.cores=parallel::detectCores ())

data_pilot01 <- 
  read_delim("Data/TextViz.csv", delim = ";")|> 
  # compute survey duration
  mutate(survey_duration = dmy_hm(ended) - dmy_hm(created)) |> 
  # rename true effect sizes
  mutate(text_effect_size_1 = effect_size_7,
         text_effect_size_2 = effect_size_8,
         text_effect_size_3 = effect_size_9,
         text_effect_size_4 = effect_size_10,
         text_effect_size_5 = effect_size_11,
         text_effect_size_6 = effect_size_12,
         plot_effect_size_1 = effect_size_1,
         plot_effect_size_2 = effect_size_2,
         plot_effect_size_3 = effect_size_3,
         plot_effect_size_4 = effect_size_4,
         plot_effect_size_5 = effect_size_5,
         plot_effect_size_6 = effect_size_6) |> 
  # deselect old true effect size variables
  select(-starts_with("effect_size_"))

glimpse(data_pilot01)
skimr::skim(data_pilot01)
```

## Participant Behaviour

### Survey Duration

```{r}
data_pilot01  |> 
  ggplot(aes(survey_duration)) +
  geom_histogram() +
  theme_minimal()

```

## Results of Attention and Comprehension checks

```{r}
data_pilot01 |> 
  select(attention_check_1a, attention_check_1b,
         check_graph2, check_test, check_text2) |> 
  skim()
```

## Data Wrangling Wide to Long Format

It is important to know that `effect_size1` to `effect_size6` encode the true effect sizes of the shown plots. `effect_size7` to `effect_size12` encode the true effect sizes of the shown **text**.

First we have to wrangle a table like this:

| session | stimulus_type | stimulus_number | true_effectsize | first_stim | perceived_diff | perceived_PS | perceived_inf | perceived_rel |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| a       | plot          | 1               | .8              | plot_first |                |              |               |               |
| b       | text          | 1               | .2              | text_first |                |              |               |               |
| c       | plot          | 1               | -.5             | plot_first |                |              |               |               |

The variable `first_stim` is only reconstuctable by the time stamps. So let's wrangle the first four columns:

```{r}
data_maxlong <- 
  data_pilot01 |> 
  select(session,
         starts_with("plot_"),
         starts_with("text_")) |> 
  pivot_longer(cols = -session,
               names_to = "variable",
               values_to = "value") |> 
  mutate(stimulus_type = str_sub(variable, 1, 4),
         nth_stimulus_per_type = str_sub(variable, -2, -1),
         nth_stimulus_per_type = case_when(nth_stimulus_per_type == "_1" ~ 1,
                                           nth_stimulus_per_type == "_2" ~ 2,
                                           nth_stimulus_per_type == "_3" ~ 3,
                                           nth_stimulus_per_type == "_4" ~ 4,
                                           nth_stimulus_per_type == "_5" ~ 5,
                                           nth_stimulus_per_type == "_6" ~ 6,
                                           nth_stimulus_per_type == "_7" ~ 7,
                                           nth_stimulus_per_type == "_8" ~ 8,
                                           nth_stimulus_per_type == "_9" ~ 9,
                                           nth_stimulus_per_type == "10" ~ 10,
                                           nth_stimulus_per_type == "11" ~ 11,
                                           nth_stimulus_per_type == "12" ~ 12), 
         measurement = case_when(str_detect(variable, "accuracy") ~ "probability_of_sup",
                                 str_detect(variable, "effect_size") ~ "effect_size",
                                 str_detect(variable, "informativity") ~ "informativity",
                                 str_detect(variable, "relevance") ~ "relevance",
                                 str_detect(variable, "difficulty") ~ "difficulty",
                                 T ~ "Attention: Error in Code"))

data_long <- 
  data_maxlong |> 
  select(-variable) |> 
  pivot_wider(id_cols = c(session, stimulus_type, nth_stimulus_per_type),
              names_from = measurement,
              values_from = value) 

skim(data_long)  
```

## Research Questions

RQ 1: How accurate (dependent variable 1), informative (dependent variable 2), difficult to understand / efficient (dependent variable 3) and relevant (dependent variable 4) are graphically and textually presented effect sizes perceived by teachers?

RQ 2: Are there any (significant) differences between the two modes (graph vs. text) in relation to the four dependent variables?

## Transformation of accuracy variable

```{r}

data_long <- 
  data_long |> 
  # we recognized that our stimulus is the other way around, that's why we
  # have to recode the effect sizes variable
  mutate(#effect_size_pos = d_to_p_superiority(effect_size), 
         #effect_size_pos = 1-(effect_size_pos),
         #effect_size_pos = 100*(effect_size_pos),
         #probability_of_sup_diff = probability_of_sup - effect_size_pos,
         rating_in_d = sqrt(2)*qnorm(probability_of_sup/100),
         overunderrating = ifelse(effect_size < 0, 
                                  (rating_in_d - effect_size)*(-1), 
                                  rating_in_d - effect_size),
         relevance_obj = case_when(effect_size > 0 ~ 0,
                                   T ~ abs(effect_size))) |> 
  # person specific standardization of relevance
  group_by(session) |> 
  mutate(relevance_pstand = (relevance - mean(relevance, na.rm = T))/
           sd(relevance, na.rm = T)) |> 
  ungroup()
```

## Analysis per dependent variables
### Informativity
#### Plot

```{r}
data_long |>
  ggplot(aes(stimulus_type, informativity)) +
  geom_violin() +
  geom_sina() +
  stat_summary(fun.data = "mean_sdl",
               fun.args = list(mult = 1)) +
  theme_minimal() +
  theme(legend.position = "none")
```

#### Model
```{r}
mod0_informativity <- brm(informativity ~ + (1|session), 
                          data = data_long,
                          cores = 4)

mod1_informativity <- brm(informativity ~ stimulus_type + (1|session), 
                          data = data_long,
                          iter = 40000,
                          cores = 4)


summary(mod1_informativity)
pp_check(mod1_informativity)
hypothesis(mod1_informativity, "stimulus_typetext > 0")

# sjPlot::tab_model(mod0_informativity, mod1_informativity, mod2_informativity)

```

#### Effect size 
```{r}
cliffs_delta(informativity ~ stimulus_type, data = data_long)
```

### Accuracy: Over/underatings
#### Plot

```{r}
# Overunderrating per stimulus type
data_long |> 
    ggplot(aes(stimulus_type, overunderrating)) +
    geom_boxplot() +
    geom_jitter() +
    stat_smooth() +
    theme_minimal()

data_long |> 
  ggplot(aes(overunderrating)) +
  geom_histogram() +
  facet_wrap(~stimulus_type)
```

#### Model

```{r}
mod1_overunderrating <-
  brm(
    overunderrating ~ stimulus_type + (1 | session),
    data = data_long |>
      filter(!overunderrating == "-Inf"),
    iter = 40000,
    cores = 4
  )

summary(mod1_overunderrating)
pp_check(mod1_overunderrating)
hypothesis(mod1_overunderrating, "Intercept < 0")
hypothesis(mod1_overunderrating, "stimulus_typetext < 0")

mod2_overunderrating <-
  brm(
    bf(overunderrating ~ stimulus_type + (1 | session),
       sigma ~ stimulus_type + (1|session)),
    data = data_long |>
      filter(!overunderrating == "-Inf"),
    cores = 4
  )

plot(mod2_overunderrating)
summary(mod2_overunderrating)
pp_check(mod2_overunderrating)
hypothesis(mod2_overunderrating, "Intercept < 0")
hypothesis(mod2_overunderrating, "stimulus_typetext < 0")
hypothesis(mod2_overunderrating, "exp(sigma_Intercept) - exp(sigma_Intercept + sigma_stimulus_typetext) < 0")
```

#### Effect size 
```{r}
cliffs_delta(overunderrating ~ stimulus_type, data = data_long)
cohens_d(overunderrating ~ stimulus_type, data = data_long |> 
           filter(!overunderrating == "-Inf"))
```

### Difficulty
#### Plot

```{r}
data_long |>
  ggplot(aes(stimulus_type, difficulty)) +
  geom_violin() +
  geom_sina() +
  stat_summary(
    fun.data = "mean_sdl",
    fun.args = list(mult = 1)
  ) + 
  theme_minimal() +
  theme(legend.position = "none")
```

#### Model
```{r}
mod1_difficulty <-
  brm(
    difficulty ~ stimulus_type + (1|session),
    family = mixture(gaussian, gaussian),
    data = data_long,
    init = 0,
    iter = 20000,
    prior = c(
      prior(normal(2, 1), Intercept, dpar = mu1),
      prior(normal(6, 1), Intercept, dpar = mu2)
    )
  )

summary(mod1_difficulty)
pp_check(mod1_difficulty)
hypothesis(mod1_difficulty, "mu1_Intercept < mu2_Intercept")
hypothesis(mod1_difficulty, "mu1_Intercept*theta1 + mu1_stimulus_typetext*theta2 < mu2_Intercept**theta1 + mu2_stimulus_typetext*theta2")


mod2_difficulty <-
  brm(
    difficulty ~ stimulus_type + (1|session),
    family = cumulative(),
    data = data_long,
    init = 0
  )

summary(mod2_difficulty)
pp_check(mod2_difficulty)
hypothesis(mod2_difficulty, "stimulus_typetext > 0")
```

#### Effect Size

```{r}
cliffs_delta(difficulty ~ stimulus_type, data = data_long)
effectsize::rank_biserial(difficulty ~ stimulus_type, data = data_long)
```

### Relevance

#### Plot

```{r}
data_long |> 
    ggplot(aes(stimulus_type, relevance)) +
    geom_jitter() +
    theme_minimal()

# it seems like some participants have always used the same number 
# that is why we plot relevance per condition and person
data_long |>
  ggplot(aes(stimulus_type, 
             relevance, 
             color = session)) +
  geom_jitter(alpha = .6) +
  facet_wrap(~effect_size) + 
  theme_minimal() +
  theme(legend.position = "none")
```

##### Log Relevance per condition

```{r}
data_long |>
  ggplot(aes(stimulus_type, 
             log(relevance))) +
  geom_jitter(aes(color = session),
              alpha = .6, width = .1) +
  stat_summary(fun.data = "mean_sdl",
               fun.args = list(mult = 1)) +
  facet_wrap(~effect_size) + 
  theme_minimal() +
  theme(legend.position = "none")
```

##### Relevance per nth-stimulus

```{r}
data_long |> 
  ggplot(aes(nth_stimulus_per_type, 
             log(relevance +1), 
             color = session,
             group = session)) +
  geom_point(aes(size = relevance_obj)) +
  geom_line() +
  facet_wrap(~ stimulus_type) +
  theme_minimal() +
  theme(legend.position = "none")
```

##### Personwise standardized relevance per true effect size

```{r}
data_long |>
  ggplot(aes(stimulus_type, 
             relevance_pstand)) +
  geom_jitter(alpha = .6, width = .1) +
  stat_summary(fun.data = "mean_sdl",
               fun.args = list(mult = 1)) +
  facet_wrap(~effect_size) + 
  geom_hline(yintercept = 0, color = "#8cd000") +
  theme_minimal()

data_long |> 
  ggplot(aes(relevance_obj, relevance_pstand)) + 
  geom_jitter() +
  stat_smooth() +
  theme_minimal()

data_long |> 
  ggplot(aes(relevance_obj, relevance_pstand, color = stimulus_type)) + 
  geom_jitter() +
  stat_smooth() +
  theme_minimal()
```

#### Model 

```{r}
mod1_relevance_pstandplot <-
  brm(
    relevance_pstand ~ relevance_obj + 
                       stimulus_type + 
                       stimulus_type:relevance_obj + 
                       (1 | session),
    data = data_long,
    cores = 4
  )

summary(mod1_relevance_pstandplot)
pp_check(mod1_relevance_pstandplot)
hypothesis(mod1_relevance_pstandplot, "stimulus_typetext > 0")
```


#### Effect Size 

```{r}
cliffs_delta(relevance ~ stimulus_type, data = data_long)
#kendalls tau berechnen

data_long |> 
    group_by(stimulus_type) |> 
    do(tau_accuracy = unlist(cor(.$relevance_obj, .$relevance_pstand, method = "kendall", 
                           use = "pairwise.complete"))) %>% 
    unnest(tau_accuracy)
```

